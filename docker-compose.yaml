version: '3.8'

services:
  llama_fastapi:
    build:
      context: .
      dockerfile: docker/mt-sentigen-llm-inference.Dockerfile
    ports:
      - "8000:8000"
    # command: # Optional but can use to overwrite with Gunicorn to manage Uvicorn workers
    # volumes: # Optional
