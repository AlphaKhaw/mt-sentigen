{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Experiment - GPT4All\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Import Packages](#1-import-packages)    \n",
    "2. [Load Data](#2-load-data)\n",
    "3. [Evaluation Function](#3-evaluation-function)\n",
    "4. [Models](#4-models)\n",
    "5. [Experiment - Llama-2-7B-Chat-GGML](#5-experiment---llama-2-7b-chat-ggml)\n",
    "6. [Experiment - GPT4All Falcon](#6-experiment---gpt4all-falcon)\n",
    "7. [Experiment - Hermes](#7-experiment---hermes)\n",
    "8. [Experiment - Mini Orca (Large)](#8-experiment---mini-orca-large)\n",
    "9. [Overall Results](#9-overall-results)\n",
    "10. [Insights](#10-insights)\n",
    "\n",
    "### Objective\n",
    "\n",
    "The primary goal of this notebook is to test the feasibility of utilising Large Language Model (LLM) for our multitask objectives of sentiment classification and response generation. GPT4All library, by Nomic AI, is chosen as it provides Python bindings and provides an ecosystem of open-source LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gpt4all import GPT4All\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from textstat import flesch_reading_ease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1783813</th>\n",
       "      <td>2</td>\n",
       "      <td>AYCE Shrimp good value if you like the flavors...</td>\n",
       "      <td>Were so glad you had a great experience at our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035152</th>\n",
       "      <td>2</td>\n",
       "      <td>Dwayne was our salesman and he really put fort...</td>\n",
       "      <td>Hello Barbara, thank you we appreciate your fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946095</th>\n",
       "      <td>2</td>\n",
       "      <td>Good value on cabinets and flooring .Very nice...</td>\n",
       "      <td>Thank you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371890</th>\n",
       "      <td>2</td>\n",
       "      <td>Great atmosphere, knowledgeable, and friendly....</td>\n",
       "      <td>Thank you much appreciated brother see you bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962314</th>\n",
       "      <td>2</td>\n",
       "      <td>Casual atmosphere with great steak and sides. ...</td>\n",
       "      <td>Thank you, Jake! So glad you love our place! W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467452</th>\n",
       "      <td>0</td>\n",
       "      <td>If I could give this office zero stars, I woul...</td>\n",
       "      <td>Were sorry to hear about your negative experie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920144</th>\n",
       "      <td>2</td>\n",
       "      <td>They really get to know your animal! They love...</td>\n",
       "      <td>Hi Kristen Thank you so much for sharing your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125892</th>\n",
       "      <td>2</td>\n",
       "      <td>Nick is amazing, as well as the rest of the st...</td>\n",
       "      <td>Thank you for your kind review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80766</th>\n",
       "      <td>2</td>\n",
       "      <td>Best place for equine stuff</td>\n",
       "      <td>Thanks Lisa...we appreciate the FIVE Star Revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3572085</th>\n",
       "      <td>2</td>\n",
       "      <td>No indoor dining. Great food, terrific Jamoca ...</td>\n",
       "      <td>Thank you for reaching out to us.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271583</th>\n",
       "      <td>2</td>\n",
       "      <td>The goods you come to love</td>\n",
       "      <td>Thank you very much for such a great review! W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663165</th>\n",
       "      <td>2</td>\n",
       "      <td>Nice young lady explained everything. Food coo...</td>\n",
       "      <td>Thanks for letting us know! We will pass that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512529</th>\n",
       "      <td>2</td>\n",
       "      <td>Good food and good service.</td>\n",
       "      <td>Thanks for stopping in!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721375</th>\n",
       "      <td>2</td>\n",
       "      <td>I just started my therapy at Spooner but I can...</td>\n",
       "      <td>Belen, thank you for highly recommending Spoon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259554</th>\n",
       "      <td>1</td>\n",
       "      <td>It was ok</td>\n",
       "      <td>We appreciate your feedback.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254551</th>\n",
       "      <td>2</td>\n",
       "      <td>Very good food and service.</td>\n",
       "      <td>Thank you very much.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760092</th>\n",
       "      <td>2</td>\n",
       "      <td>This establishment has been there for 20 years...</td>\n",
       "      <td>Keith .. I am impressed with you food and liba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836195</th>\n",
       "      <td>2</td>\n",
       "      <td>Little crowded but friendly people</td>\n",
       "      <td>Thanks for the review, Sandra. Well share this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494501</th>\n",
       "      <td>2</td>\n",
       "      <td>Bought my Sirrus from these guys! Super knowle...</td>\n",
       "      <td>Congrats on the Sirrus Matt, such an awesome b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2676319</th>\n",
       "      <td>2</td>\n",
       "      <td>Great service</td>\n",
       "      <td>We appreciate the feedback, Art! Great service...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating                                               text  \\\n",
       "1783813       2  AYCE Shrimp good value if you like the flavors...   \n",
       "2035152       2  Dwayne was our salesman and he really put fort...   \n",
       "2946095       2  Good value on cabinets and flooring .Very nice...   \n",
       "1371890       2  Great atmosphere, knowledgeable, and friendly....   \n",
       "962314        2  Casual atmosphere with great steak and sides. ...   \n",
       "2467452       0  If I could give this office zero stars, I woul...   \n",
       "920144        2  They really get to know your animal! They love...   \n",
       "3125892       2  Nick is amazing, as well as the rest of the st...   \n",
       "80766         2                        Best place for equine stuff   \n",
       "3572085       2  No indoor dining. Great food, terrific Jamoca ...   \n",
       "1271583       2                         The goods you come to love   \n",
       "1663165       2  Nice young lady explained everything. Food coo...   \n",
       "1512529       2                        Good food and good service.   \n",
       "1721375       2  I just started my therapy at Spooner but I can...   \n",
       "1259554       1                                         It was ok    \n",
       "1254551       2                        Very good food and service.   \n",
       "1760092       2  This establishment has been there for 20 years...   \n",
       "1836195       2                 Little crowded but friendly people   \n",
       "2494501       2  Bought my Sirrus from these guys! Super knowle...   \n",
       "2676319       2                                      Great service   \n",
       "\n",
       "                                                      resp  \n",
       "1783813  Were so glad you had a great experience at our...  \n",
       "2035152  Hello Barbara, thank you we appreciate your fe...  \n",
       "2946095                                         Thank you!  \n",
       "1371890  Thank you much appreciated brother see you bac...  \n",
       "962314   Thank you, Jake! So glad you love our place! W...  \n",
       "2467452  Were sorry to hear about your negative experie...  \n",
       "920144   Hi Kristen Thank you so much for sharing your ...  \n",
       "3125892                     Thank you for your kind review  \n",
       "80766    Thanks Lisa...we appreciate the FIVE Star Revi...  \n",
       "3572085                  Thank you for reaching out to us.  \n",
       "1271583  Thank you very much for such a great review! W...  \n",
       "1663165  Thanks for letting us know! We will pass that ...  \n",
       "1512529                            Thanks for stopping in!  \n",
       "1721375  Belen, thank you for highly recommending Spoon...  \n",
       "1259554                       We appreciate your feedback.  \n",
       "1254551                               Thank you very much.  \n",
       "1760092  Keith .. I am impressed with you food and liba...  \n",
       "1836195  Thanks for the review, Sandra. Well share this...  \n",
       "2494501  Congrats on the Sirrus Matt, such an awesome b...  \n",
       "2676319  We appreciate the feedback, Art! Great service...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataframe = pd.read_csv(\"../data/split/test.csv\")\n",
    "subset_test_dataframe = test_dataframe.sample(n=20, random_state=42)\n",
    "subset_test_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentiment(\n",
    "    df: pd.DataFrame, true_labels_col: str, predicted_labels_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates the sentiment classification performance by calculating \n",
    "    the accuracy and F1 score based on true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the true and predicted \n",
    "            labels.\n",
    "        true_labels_col (str): The name of the column containing the true \n",
    "            labels.\n",
    "        predicted_labels_col (str): The name of the column containing the \n",
    "            predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with added 'Accuracy' and 'F1_Score' \n",
    "            columns.\n",
    "    \"\"\"\n",
    "    df['Accuracy'] = accuracy_score(\n",
    "        df[true_labels_col], df[predicted_labels_col]\n",
    "    )\n",
    "    df['F1_Score'] = f1_score(\n",
    "        df[true_labels_col], df[predicted_labels_col], average='weighted'\n",
    "    )\n",
    "    print(f\"Overall Accuracy: {df['Accuracy'].mean()*100:.2f}%\")\n",
    "    print(f\"Overall F1 Score: {df['F1_Score'].mean()*100:.2f}%\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def average_sentence_length(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the average sentence length of a text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        float: The average sentence length.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sum(\n",
    "        len(word_tokenize(sentence)) for sentence in sentences\n",
    "    ) / len(sentences)\n",
    "\n",
    "def average_word_length(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the average word length of a text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        float: The average word length.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "def lexical_diversity(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the lexical diversity of a text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        float: The lexical diversity.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "def evaluate_generated_response(\n",
    "    df: pd.DataFrame, response_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates the quality of generated text responses by calculating \n",
    "    readability, fluency, and complexity metrics.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the generated responses.\n",
    "        response_col (str): The name of the column containing the generated \n",
    "            responses.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with added evaluation metrics as columns.\n",
    "    \"\"\"\n",
    "    readability_scores = []\n",
    "    avg_sentence_lengths = []\n",
    "    avg_word_lengths = []\n",
    "    lexical_diversities = []\n",
    "\n",
    "    for response in df[response_col]:\n",
    "        # Evaluate Readability\n",
    "        readability_scores.append(flesch_reading_ease(response))\n",
    "        \n",
    "        # Evaluate Complexity\n",
    "        avg_sentence_lengths.append(average_sentence_length(response))\n",
    "        avg_word_lengths.append(average_word_length(response))\n",
    "        lexical_diversities.append(lexical_diversity(response))\n",
    "\n",
    "    df['Readability_Score'] = readability_scores\n",
    "    df['Avg_Sentence_Length'] = avg_sentence_lengths\n",
    "    df['Avg_Word_Length'] = avg_word_lengths\n",
    "    df['Lexical_Diversity'] = lexical_diversities\n",
    "\n",
    "    # Calculate and print overall scores\n",
    "    print(f\"Overall Readability Score: {df['Readability_Score'].mean():.2f}\")\n",
    "    print(f\"Overall Average Sentence Length: \")\n",
    "    print(f\"{df['Avg_Sentence_Length'].mean():.2f}\")\n",
    "    print(f\"Overall Average Word Length: {df['Avg_Word_Length'].mean():.2f}\")\n",
    "    print(f\"Overall Lexical Diversity: {df['Lexical_Diversity'].mean():.2f}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Models\n",
    "\n",
    "a. **Wizard v1.1 - 13B**   \n",
    "b. **GPT4All Falcon - 7B**   \n",
    "c. **Hermes - 13B**   \n",
    "d. **Snoozy - 13B**      \n",
    "e. **Mini Orca - 7B**     \n",
    "f. **Mini Orca (Small) - 3B**    \n",
    "g. **Mini Orca (Large) - 7B**    \n",
    "h. **Wizard Uncensored - 13B**    \n",
    "i. **Replit - 3B**    \n",
    "j. **Starcoder (Small) - 3B**    \n",
    "k. **Starcoder - 7B**    \n",
    "l. **Llama-2-7B Chat**     \n",
    "\n",
    "<div style=\"background-color: black; color: white; padding: 10px\">\n",
    "    <p><b>In this notebook, I will experiment with a couple of the available models to test the prompt template as well as finalize any post-processing steps required:\n",
    "\n",
    "1. Llama2-7B Chat\n",
    "2. GPT4All Falcon\n",
    "3. Hermes\n",
    "4. Mini Orca (Large)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment - Llama-2-7B-Chat-GGML\n",
    "\n",
    "First, we experiment with LlaMA2 model from Meta AI. It is fine-tuned for dialogue.\n",
    "\n",
    "**Hugging Face Model card** - https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/Alpha/.cache/gpt4all/llama-2-7b-chat.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: using Metal\n",
      "llama.cpp: loading model from /Users/Alpha/.cache/gpt4all/llama-2-7b-chat.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_head_kv  = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.08 MB\n",
      "llama_model_load_internal: mem required  = 4013.73 MB (+ 1024.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/opt/homebrew/Caskroom/miniforge/base/envs/mt-sentigen/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x17ea3fa00\n",
      "ggml_metal_init: loaded kernel_add_row                        0x17ea41be0\n",
      "ggml_metal_init: loaded kernel_mul                            0x17ea40e70\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x17ea410d0\n",
      "ggml_metal_init: loaded kernel_scale                          0x17ea429e0\n",
      "ggml_metal_init: loaded kernel_silu                           0x17ea43070\n",
      "ggml_metal_init: loaded kernel_relu                           0x17ea41330\n",
      "ggml_metal_init: loaded kernel_gelu                           0x17ea43d70\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x17ea443b0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x17ea44840\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x17ea452f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x17ea45920\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x17ea46900\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x17ea46b60\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x17ea46fc0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x17ea476e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x17d6ad690\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x17d6ae2d0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x17d6aeec0\n",
      "ggml_metal_init: loaded kernel_norm                           0x17d6af5e0\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x17ea48170\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x17ea483d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x17ea487d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x16d338250\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x16d3390a0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x16c1dc000\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x102c21140\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x102c231e0\n",
      "ggml_metal_init: loaded kernel_rope                           0x102c208d0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x16d339300\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x16d339560\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x16c1dc260\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x16c1dd5c0\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    70.31 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  3616.08 MB, ( 3616.53 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    10.17 MB, ( 3626.70 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1026.00 MB, ( 4652.70 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   228.00 MB, ( 4880.70 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   160.00 MB, ( 5040.70 / 21845.34)\n"
     ]
    }
   ],
   "source": [
    "model = GPT4All(\"llama-2-7b-chat.ggmlv3.q4_0.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt function to perform Sentiment Classification and Response Generation Task**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_llama_prompt(review: str) -> str:\n",
    "    prompt = (\n",
    "        \"[INST] <<SYS>>\\n\"\n",
    "        \"You are a helpful and attentive business owner who always \"\n",
    "        \"appreciates customer feedback. If the customer had a positive \"\n",
    "        \"experience, make sure to thank them. If the customer had a \"\n",
    "        \"negative or neutral experience, apologize and address their \"\n",
    "        \"concerns respectfully and constructively. \"\n",
    "        \"Do limit your response to 3 sentences.\\n\"\n",
    "        \"<</SYS>>\\n\\n\"\n",
    "        \"Please classify the following customer review as either \"\n",
    "        f\"\\\"Positive,\\\" \\\"Negative,\\\" or \\\"Neutral\\\" and provide a business \"\n",
    "        f\"owner's response accordingly. The customer review is: \\\"{review}\\\"\\n\"\n",
    "        \"Format your output in JSON format: \"\n",
    "        \"{\\\"sentiment\\\": \\\"<Sentiment>\\\", \\\"response\\\": \\\"<Response>\\\"}\\n\"\n",
    "        \"[/INST]\\n\\n \"\n",
    "    )\n",
    "\n",
    "    output = model.generate(prompt, max_tokens=150)\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_test_dataframe['output'] = subset_test_dataframe['text'].apply(\n",
    "    use_llama_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post Processing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_test_dataframe['output'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x if x.endswith(\"}\") else x + \"}\"\n",
    ")\n",
    "subset_test_dataframe['output'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: eval(x)\n",
    ")\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['sentiment']\n",
    ")\n",
    "subset_test_dataframe['response'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['response']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print output samples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0:\n",
      "{'sentiment': 'Neutral', 'response': \"Thank you for taking the time to share your feedback! We appreciate your input and are glad to hear that our AYCE shrimp is a good value. However, we apologize for any inconvenience caused by the service. Please let us know if there's anything else we can do to improve your experience.\"}\n",
      "Index 1:\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to leave us a review! We're thrilled to hear that Dwayne provided excellent service and helped you find the perfect vehicle. Your satisfaction is our top priority, and we appreciate your business!\"}\n",
      "Index 2:\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to leave us a review! We're thrilled to hear that you found our cabinets and flooring to be good value. Our team works hard to provide excellent service, and we appreciate your recognition. Please don't hesitate to reach out if there's anything else we can help with!\"}\n",
      "Index 3:\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to leave us this review! We're thrilled to hear that you had a great experience at our shop, especially with all the handsome men walking out of here. Your support means everything to us and we can't wait to continue serving you in the future!\"}\n",
      "Index 4:\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to review our restaurant! We're thrilled to hear that you and your family had a great experience with us, including enjoying our delicious steak and sides. It's always wonderful to meet one of our owners and have them make your visit extra special. We appreciate your business and hope to see you again soon!\"}\n"
     ]
    }
   ],
   "source": [
    "for i, output in enumerate(subset_test_dataframe['output'].iloc[:5]):\n",
    "    print(f\"Index {i}:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 85.00%\n",
      "Overall F1 Score: 87.65%\n",
      "Overall Readability Score: 76.75\n",
      "Overall Average Sentence Length: \n",
      "16.98\n",
      "Overall Average Word Length: 4.05\n",
      "Overall Lexical Diversity: 0.81\n"
     ]
    }
   ],
   "source": [
    "# SENTIMENT_MAPPING = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "SENTIMENT_MAPPING = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['sentiment']\n",
    ")\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['sentiment'].map(\n",
    "    SENTIMENT_MAPPING\n",
    ")\n",
    "subset_test_dataframe['response'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['response']\n",
    ")\n",
    "subset_test_dataframe = evaluate_sentiment(\n",
    "    subset_test_dataframe, 'rating', 'sentiment'\n",
    ")\n",
    "subset_test_dataframe = evaluate_generated_response(\n",
    "    subset_test_dataframe, 'response'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment - GPT4All Falcon\n",
    "\n",
    "Next, we experiment with GPT4All Falcon which is fine-tuned by Nomic AI. It is described by `Nomic AI` as the best overall smaller model with fast responses.\n",
    "\n",
    "**Hugging Face Model card** - https://huggingface.co/nomic-ai/gpt4all-falcon-ggml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model file at  /Users/Alpha/.cache/gpt4all/ggml-model-gpt4all-falcon-q4_0.bin\n",
      "falcon_model_load: loading model from '/Users/Alpha/.cache/gpt4all/ggml-model-gpt4all-falcon-q4_0.bin' - please wait ...\n",
      "falcon_model_load: n_vocab   = 65024\n",
      "falcon_model_load: n_embd    = 4544\n",
      "falcon_model_load: n_head    = 71\n",
      "falcon_model_load: n_head_kv = 1\n",
      "falcon_model_load: n_layer   = 32\n",
      "falcon_model_load: ftype     = 2\n",
      "falcon_model_load: qntvr     = 0\n",
      "falcon_model_load: ggml ctx size = 3872.64 MB\n",
      "falcon_model_load: memory_size =    32.00 MB, n_mem = 65536\n",
      "falcon_model_load: ........................ done\n",
      "falcon_model_load: model size =  3872.59 MB / num tensors = 196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "model = GPT4All(\"ggml-model-gpt4all-falcon-q4_0.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt function to perform Sentiment Classification and Response Generation Task**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_falcon_prompt(review: str) -> str:\n",
    "    instruction = (\n",
    "        \"You are a helpful and attentive business owner who always \"\n",
    "        \"appreciates customer feedback. If the customer had a positive \"\n",
    "        \"experience, make sure to thank them. If the customer had a \"\n",
    "        \"negative or neutral experience, apologize and address their \"\n",
    "        \"concerns respectfully and constructively. \"\n",
    "        \"Do limit your response to 3 sentences.\\n\\n\"\n",
    "        \"Please classify the following customer review as either \"\n",
    "        \"\\\"Positive,\\\" \\\"Negative,\\\" or \\\"Neutral\\\" and provide a business \"\n",
    "        f\"owner's response accordingly. The customer review is: \\\"{review}\\\"\\n\"\n",
    "        \"Strictly format your string output in JSON format: \"\n",
    "        \"{\\\"sentiment\\\": \\\"<Sentiment>\\\", \\\"response\\\": \\\"<Response>\\\"}\"\n",
    "    )\n",
    "\n",
    "    falcon_prompt = f\"### Instruction:\\n{instruction}\\n### Response:\\n \" + \"\\{\"\n",
    "    output = model.generate(falcon_prompt, max_tokens=150)\n",
    "    \n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_test_dataframe['output'] = subset_test_dataframe['text'].apply(\n",
    "    use_llama_prompt\n",
    ")\n",
    "subset_test_dataframe['output'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x if x.endswith(\"}\") else x + \"}\"\n",
    ")\n",
    "subset_test_dataframe['output'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x if x.startswith(\"{\") else \"{\" + x\n",
    ")\n",
    "subset_test_dataframe['output'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: eval(x)\n",
    ")\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['sentiment']\n",
    ")\n",
    "subset_test_dataframe['response'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['response']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print output samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0:\n",
      "{'sentiment': 'Positive', 'response': 'Thank you for the positive feedback! We are glad to hear that you enjoyed our shrimp. If you have any further questions or concerns, please do not hesitate to contact us.'}\n",
      "Index 1:\n",
      "{'sentiment': 'Positive', 'response': 'Thank you for the positive feedback! We are glad to hear that our salesman Dwayne put forth extra effort to ensure your satisfaction with your purchase. Your feedback is valuable and helps us improve our services. If you have any further questions or concerns, please do not hesitate to contact us.'}\n",
      "Index 2:\n",
      "{'sentiment': 'Positive', 'response': \"Thank you for the positive feedback! We are glad to hear that our products and staff provided good value. If you have any further questions or concerns, please don't hesitate to contact us.\"}\n",
      "Index 3:\n",
      "{'sentiment': 'Positive', 'response': 'Thank you for the positive feedback! We are always happy to hear that our customers enjoy their experience at our shop. Your kind words mean a lot to us and we will make sure to pass them on to our team.'}\n",
      "Index 4:\n",
      "{'sentiment': 'Positive', 'response': 'Thank you for the positive feedback! We are thrilled to hear that you had a great experience with our restaurant. Your kind words mean a lot to us and we will make sure to pass them on to our team. We hope to see you again soon!'}\n"
     ]
    }
   ],
   "source": [
    "for i, output in enumerate(subset_test_dataframe['output'].iloc[:5]):\n",
    "    print(f\"Index {i}:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 95.00%\n",
      "Overall F1 Score: 92.57%\n",
      "Overall Readability Score: 76.77\n",
      "Overall Average Sentence Length: \n",
      "13.66\n",
      "Overall Average Word Length: 4.17\n",
      "Overall Lexical Diversity: 0.85\n"
     ]
    }
   ],
   "source": [
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['sentiment']\n",
    ")\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['sentiment'].map(\n",
    "    SENTIMENT_MAPPING\n",
    ")\n",
    "subset_test_dataframe['response'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['response']\n",
    ")\n",
    "subset_test_dataframe = evaluate_sentiment(\n",
    "    subset_test_dataframe, 'rating', 'sentiment'\n",
    ")\n",
    "subset_test_dataframe = evaluate_generated_response(\n",
    "    subset_test_dataframe, 'response'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Experiment - Hermes\n",
    "\n",
    "The next LLM to experiment with is Hermes which is instructed based that is curated with 300,000 uncensored instructions. Trained by Nous Research.\n",
    "\n",
    "\n",
    "**Hugging Face Model card** - https://huggingface.co/TheBloke/Nous-Hermes-13B-GGML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7.32G/7.32G [10:50<00:00, 11.3MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded at:  /Users/Alpha/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: using Metal\n",
      "llama.cpp: loading model from /Users/Alpha/.cache/gpt4all/nous-hermes-13b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7477.73 MB (+ 1600.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1600.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/opt/homebrew/Caskroom/miniforge/base/envs/mt-sentigen/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x2f048e670\n",
      "ggml_metal_init: loaded kernel_add_row                        0x2f048e8d0\n",
      "ggml_metal_init: loaded kernel_mul                            0x2f048eb30\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x2f048ed90\n",
      "ggml_metal_init: loaded kernel_scale                          0x2f048eff0\n",
      "ggml_metal_init: loaded kernel_silu                           0x2f048f250\n",
      "ggml_metal_init: loaded kernel_relu                           0x2f048f4b0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x2f048f710\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x2f048f970\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x2f048fbd0\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x2f048fe30\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x2f0490090\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x2f04902f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x2f0490550\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x2f04907b0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x2f0490a10\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x2f0490c70\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x2f0490ed0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x2f0491130\n",
      "ggml_metal_init: loaded kernel_norm                           0x2f0491390\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x2f04915f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x2f0491850\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x2f0491ab0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x2f0491d10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x2f0491f70\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x2f04921d0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x2f0492430\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x2f0492690\n",
      "ggml_metal_init: loaded kernel_rope                           0x2f04928f0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x2f0492b50\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x2f0492db0\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x2f0493010\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x2f0493270\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.06 MB, ( 6985.75 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, ( 6997.92 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1602.00 MB, ( 8599.92 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   290.00 MB, ( 8889.92 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, ( 9081.92 / 21845.34)\n"
     ]
    }
   ],
   "source": [
    "model = GPT4All(\"nous-hermes-13b.ggmlv3.q4_0.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt function to perform Sentiment Classification and Response Generation Task**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_hermes_prompt(review):\n",
    "    instruction = (\n",
    "        \"You are a helpful and attentive business owner who always \"\n",
    "        \"appreciates customer feedback. If the customer had a positive \"\n",
    "        \"experience, make sure to thank them. If the customer had a \"\n",
    "        \"negative or neutral experience, apologize and address their \"\n",
    "        \"concerns respectfully and constructively. \"\n",
    "        \"Do limit your response to 3 sentences.\\n\\n\"\n",
    "        \"Please classify the following customer review as either \"\n",
    "        \"\\\"Positive,\\\" \\\"Negative,\\\" or \\\"Neutral\\\" and provide a business \"\n",
    "        f\"owner's response accordingly. The customer review is: \\\"{review}\\\"\\n\"\n",
    "        \"Format your output in JSON format: \"\n",
    "        \"{\\\"sentiment\\\": \\\"<Sentiment>\\\", \\\"response\\\": \\\"<Response>\\\"}\"\n",
    "    )\n",
    "\n",
    "    hermes_prompt = f\"### Instruction:\\n{instruction}\\n### Response:\\n\"\n",
    "    \n",
    "    output = model.generate(hermes_prompt, max_tokens=150)\n",
    "    \n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_test_dataframe['output'] = subset_test_dataframe['text'].apply(\n",
    "    use_hermes_prompt\n",
    ")\n",
    "subset_test_dataframe['output'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x if x.endswith(\"}\") else x + \"}\"\n",
    ")\n",
    "subset_test_dataframe['output'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: eval(x)\n",
    ")\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['sentiment']\n",
    ")\n",
    "subset_test_dataframe['response'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['response']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print output samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0:\n",
      "{'sentiment': 'Positive', 'response': \"Thank you for your feedback! We're glad to hear that our AYCE Shrimp is good value. We will work on improving our service to ensure a better experience next time.\"}\n",
      "Index 1:\n",
      "{'sentiment': 'Positive', 'response': \"Thank you for taking the time to share your positive experience with us! We're glad Dwayne was able to help you find a great vehicle, and we appreciate your kind words about our selection. Thank you again for choosing our dealership!\"}\n",
      "Index 2:\n",
      "{'sentiment': 'Positive', 'response': \"Thank you for taking the time to share your experience with us! We're glad that you found our products and staff to be of good value. Your feedback is appreciated, and we hope to continue providing excellent service in the future.\"}\n",
      "Index 3:\n",
      "{'sentiment': 'Positive', 'response': \"Thank you for taking the time to share your experience! We're thrilled that our atmosphere, knowledgeable staff and friendly service made your visit a positive one. Please don't hesitate to reach out if there is anything else we can do to make your next visit even better.\"}\n",
      "Index 4:\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to share your experience with us! We're thrilled that you enjoyed our steak and casual atmosphere, as well as meeting one of our owners. It means a lot to us that we were able to help make your celebration special. Thank you again!\"}\n"
     ]
    }
   ],
   "source": [
    "for i, output in enumerate(subset_test_dataframe['output'].iloc[:5]):\n",
    "    print(f\"Index {i}:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 95.00%\n",
      "Overall F1 Score: 93.33%\n",
      "Overall Readability Score: 76.04\n",
      "Overall Average Sentence Length: \n",
      "15.46\n",
      "Overall Average Word Length: 4.07\n",
      "Overall Lexical Diversity: 0.85\n"
     ]
    }
   ],
   "source": [
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['sentiment']\n",
    ")\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['sentiment'].map(\n",
    "    SENTIMENT_MAPPING\n",
    ")\n",
    "subset_test_dataframe['response'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['response']\n",
    ")\n",
    "subset_test_dataframe = evaluate_sentiment(\n",
    "    subset_test_dataframe, 'rating', 'sentiment'\n",
    ")\n",
    "subset_test_dataframe = evaluate_generated_response(\n",
    "    subset_test_dataframe, 'response'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Experiment - Mini Orca (Large)\n",
    "\n",
    "The next LLM is Mini Orca (Large). GPT4All offers several variation of sizes for Mini Orca - namely 3B, 7B and 13B. The Mini Orca (Large) with 13B parameters will be chosen to represent the entire catalogue of Mini Orca LLM. \n",
    "\n",
    "**Hugging Face Model card** - https://huggingface.co/TheBloke/orca_mini_13B-GGML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7.32G/7.32G [16:17<00:00, 7.49MiB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model downloaded at:  /Users/Alpha/.cache/gpt4all/orca-mini-13b.ggmlv3.q4_0.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: using Metal\n",
      "llama.cpp: loading model from /Users/Alpha/.cache/gpt4all/orca-mini-13b.ggmlv3.q4_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 5120\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 40\n",
      "llama_model_load_internal: n_head_kv  = 40\n",
      "llama_model_load_internal: n_layer    = 40\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: n_gqa      = 1\n",
      "llama_model_load_internal: rnorm_eps  = 5.0e-06\n",
      "llama_model_load_internal: n_ff       = 13824\n",
      "llama_model_load_internal: freq_base  = 10000.0\n",
      "llama_model_load_internal: freq_scale = 1\n",
      "llama_model_load_internal: ftype      = 2 (mostly Q4_0)\n",
      "llama_model_load_internal: model size = 13B\n",
      "llama_model_load_internal: ggml ctx size =    0.11 MB\n",
      "llama_model_load_internal: mem required  = 7477.72 MB (+ 1600.00 MB per state)\n",
      "llama_new_context_with_model: kv self size  = 1600.00 MB\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: using MPS\n",
      "ggml_metal_init: loading '/opt/homebrew/Caskroom/miniforge/base/envs/mt-sentigen/lib/python3.10/site-packages/gpt4all/llmodel_DO_NOT_MODIFY/build/ggml-metal.metal'\n",
      "ggml_metal_init: loaded kernel_add                            0x2ee0480b0\n",
      "ggml_metal_init: loaded kernel_add_row                        0x2ee048310\n",
      "ggml_metal_init: loaded kernel_mul                            0x2ee048570\n",
      "ggml_metal_init: loaded kernel_mul_row                        0x2ee0487d0\n",
      "ggml_metal_init: loaded kernel_scale                          0x2ee048a30\n",
      "ggml_metal_init: loaded kernel_silu                           0x2ee048c90\n",
      "ggml_metal_init: loaded kernel_relu                           0x2ee048ef0\n",
      "ggml_metal_init: loaded kernel_gelu                           0x2ee049150\n",
      "ggml_metal_init: loaded kernel_soft_max                       0x2ee0493b0\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                  0x2ee049610\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                   0x2ee049870\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                  0x2ee049ad0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                  0x2ee049d30\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                  0x2ee049f90\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                  0x367c2d4e0\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                  0x2ee04a1f0\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                  0x2ee04a450\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                  0x2ee04a6b0\n",
      "ggml_metal_init: loaded kernel_rms_norm                       0x2ee04a910\n",
      "ggml_metal_init: loaded kernel_norm                           0x2ee04ab70\n",
      "ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x2ee04add0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x2ee04b030\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x2ee04b290\n",
      "ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x2ee04b4f0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x2ee04b750\n",
      "ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x2ee04b9b0\n",
      "ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x2ee04bc10\n",
      "ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x2ee04be70\n",
      "ggml_metal_init: loaded kernel_rope                           0x2ee04c0d0\n",
      "ggml_metal_init: loaded kernel_alibi_f32                      0x2ee04c330\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                    0x2ee04c590\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                    0x2ee04c7f0\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                    0x2ee04ca50\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize = 21845.34 MB\n",
      "ggml_metal_init: hasUnifiedMemory             = true\n",
      "ggml_metal_init: maxTransferRate              = built-in GPU\n",
      "llama_new_context_with_model: max tensor size =    87.89 MB\n",
      "ggml_metal_add_buffer: allocated 'data            ' buffer, size =  6984.08 MB, (16066.00 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'eval            ' buffer, size =    12.17 MB, (16078.17 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  1602.00 MB, (17680.17 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr0            ' buffer, size =   290.00 MB, (17970.17 / 21845.34)\n",
      "ggml_metal_add_buffer: allocated 'scr1            ' buffer, size =   192.00 MB, (18162.17 / 21845.34)\n",
      "ggml_metal_free: deallocating\n"
     ]
    }
   ],
   "source": [
    "model = GPT4All(\"orca-mini-13b.ggmlv3.q4_0.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt function to perform Sentiment Classification and Response Generation Task**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_mini_orca_prompt(review):\n",
    "    system = (\n",
    "        \"You are a helpful and attentive business owner who always \"\n",
    "        \"appreciates customer feedback. If the customer had a positive \"\n",
    "        \"experience, make sure to thank them. If the customer had a \"\n",
    "        \"negative or neutral experience, apologize and address their \"\n",
    "        \"concerns respectfully and constructively. \"\n",
    "        \"Do limit your response to 3 sentences.\\n\\n\"\n",
    "    )\n",
    "    instruction = (\n",
    "        \"Please classify the following customer review as either \"\n",
    "        \"\\\"Positive,\\\" \\\"Negative,\\\" or \\\"Neutral\\\" and provide a business \"\n",
    "        f\"owner's response accordingly. The customer review is: \\\"{review}\\\"\\n\"\n",
    "        \"Format your output in JSON format: \"\n",
    "        \"{\\\"sentiment\\\": \\\"<Sentiment>\\\", \\\"response\\\": \\\"<Response>\\\"}\"\n",
    "    )\n",
    "\n",
    "    hermes_prompt = (\n",
    "        f\"### System:\\n{system} \"\n",
    "        f\"### Instruction:\\n{instruction}\\n### Response:\\n\"\n",
    "    )\n",
    "\n",
    "    output = model.generate(hermes_prompt, max_tokens=150)\n",
    "\n",
    "    return output.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_test_dataframe['output'] = subset_test_dataframe['text'].apply(\n",
    "    use_mini_orca_prompt\n",
    ")\n",
    "subset_test_dataframe['output'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x if x.endswith(\"}\") else x + \"}\"\n",
    ")\n",
    "subset_test_dataframe['output'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: eval(x)\n",
    ")\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['sentiment']\n",
    ")\n",
    "subset_test_dataframe['response'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['response']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print output samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0:\n",
      "{'sentiment': 'Negative', 'response': 'Thank you for taking the time to share your feedback. We are sorry to hear that you had a negative experience with our service. Please let us know how we can improve and we will take appropriate measures to ensure that this does not happen again in the future.'}\n",
      "Index 1:\n",
      "{'sentiment': 'Positive', 'response': 'Thank you for taking the time to share your feedback with us. We are glad to hear that you had a positive experience with our salesman, Dwayne. He is always looking out for his customers and going above and beyond to ensure their satisfaction. We appreciate your business and hope to see you again soon.'}\n",
      "Index 2:\n",
      "{'sentiment': 'Positive', 'response': 'Thank you for taking the time to leave us a review. We are glad to hear that you had a positive experience with our products and services. Your satisfaction is important to us, and we will continue to strive for excellence in providing quality products and excellent customer service.'}\n",
      "Index 3:\n",
      "{'sentiment': 'Positive', 'response': 'Thank you for taking the time to share your feedback. We are glad that you had a positive experience at our shop. Your satisfaction is important to us and we will continue to strive for excellence in providing quality service to our customers.'}\n",
      "Index 4:\n",
      "{'sentiment': 'Positive', 'response': 'Thank you for taking the time to share your feedback. We are thrilled to hear that you had a positive experience at our restaurant. Your satisfaction is important to us and we will make sure to take care of you in the future.'}\n"
     ]
    }
   ],
   "source": [
    "for i, output in enumerate(subset_test_dataframe['output'].iloc[:5]):\n",
    "    print(f\"Index {i}:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 65.00%\n",
      "Overall F1 Score: 73.11%\n",
      "Overall Readability Score: 70.83\n",
      "Overall Average Sentence Length: \n",
      "14.60\n",
      "Overall Average Word Length: 4.31\n",
      "Overall Lexical Diversity: 0.82\n"
     ]
    }
   ],
   "source": [
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['sentiment']\n",
    ")\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['sentiment'].map(\n",
    "    SENTIMENT_MAPPING\n",
    ")\n",
    "subset_test_dataframe['response'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['response']\n",
    ")\n",
    "subset_test_dataframe = evaluate_sentiment(\n",
    "    subset_test_dataframe, 'rating', 'sentiment'\n",
    ")\n",
    "subset_test_dataframe = evaluate_generated_response(\n",
    "    subset_test_dataframe, 'response'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Overall Results\n",
    "\n",
    "**LlaMA2-7B Chat:**\n",
    "\n",
    "Overall Accuracy: `85.00%`   \n",
    "Overall F1 Score: `87.65%`     \n",
    "Overall Readability Score: `76.75` (Flesch Reading Ease Score - 0 to 100)\n",
    "Overall Average Sentence Length: `16.98` words per sentence   \n",
    "Overall Average Word Length: `4.05` Characters per Word    \n",
    "Overall Lexical Diversity: `0.81` (Lexical Diversity Ratio - 0 to 1)   \n",
    "Time taken: `4 minutes 45 seconds`\n",
    "\n",
    "**GPT4All Falcon:**\n",
    "\n",
    "Overall Accuracy: `95.00%`\n",
    "Overall F1 Score: `92.57%`    \n",
    "Overall Readability Score: `76.77` (Flesch Reading Ease Score - 0 to 100)   \n",
    "Overall Average Sentence Length: `13.66` words per sentence   \n",
    "Overall Average Word Length: `4.17` Characters per Word   \n",
    "Overall Lexical Diversity: `0.85` (Lexical Diversity Ratio - 0 to 1)   \n",
    "Time taken: ~8 minutes\n",
    "\n",
    "**Hermes:**\n",
    "\n",
    "Overall Accuracy: `95.00%`    \n",
    "Overall F1 Score: `93.33%`        \n",
    "Overall Readability Score: `76.04` (Flesch Reading Ease Score - 0 to 100)    \n",
    "Overall Average Sentence Length: `15.46` words per sentence   \n",
    "Overall Average Word Length: `4.07` Characters per Word   \n",
    "Overall Lexical Diversity: `0.85` (Lexical Diversity Ratio - 0 to 1)   \n",
    "Time taken: `~8 minutes`\n",
    "\n",
    "**Mini Orca (Large):**\n",
    "\n",
    "Overall Accuracy: `65.00%`     \n",
    "Overall F1 Score: `73.11%`    \n",
    "Overall Readability Score: `70.83` (Flesch Reading Ease Score - 0 to 100)    \n",
    "Overall Average Sentence Length: `14.60` words per sentence   \n",
    "Overall Average Word Length: `4.31` Characters per Word   \n",
    "Overall Lexical Diversity: `0.82` (Lexical Diversity Ratio - 0 to 1)   \n",
    "Time taken: `~8 minutes`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Insights\n",
    "\n",
    "**Best Accuracy score:** `GPT4All Falcon` & `Hermes`\n",
    "\n",
    "\n",
    "**Best F1 score:** `Hermes`\n",
    "\n",
    "\n",
    "**Best Readability score:** `LlaMa2-7B Chat`\n",
    "\n",
    "\n",
    "**Shortest Average Sentence Length:** `LlaMa2-7B Chat`\n",
    "\n",
    "\n",
    "**Longest Average Sentence Length:** `Hermes`\n",
    "\n",
    "\n",
    "**Shortest Average Word Length:** `LlaMa2-7B Chat`\n",
    "\n",
    "\n",
    "**Longest Average Word Length:** `Mini Orca (Large)`\n",
    "\n",
    "\n",
    "**Most Diverse output:** `GPT4All Falcon` & `Hermes`\n",
    "\n",
    "\n",
    "**Least Diverse output:** `LlaMa2-7B Chat`\n",
    "\n",
    "\n",
    "**Fastest Model:** `LlaMA2-7B Chat`\n",
    "\n",
    "\n",
    "**Human Evaluation of Generated Response:**\n",
    "\n",
    "\n",
    "- All the above models are able to generate proper responses corresponding to the predicted sentiment of input customer reviews\n",
    "- With the exception of `Mini Orca (Large)`, all the remaining LLMs are able to generate response containing references to the input review which adds to the personalised response and not a system generated response\n",
    "- For `LlaMA2-7B Chat` and `Hermes`, both LLMs notably reference highly specific food menu item mentioned in the input review\n",
    "- With the exception of `Mini Orca (Large)`, all the remaining LLMs achieved similar performance in Accuracy and F1 score for sentiment classification task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt-sentigen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
