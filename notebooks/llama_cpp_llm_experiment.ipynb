{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Experiment - `llama.cpp`\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Import Packages](#1-import-packages)    \n",
    "2. [Load Data](#2-load-data)\n",
    "3. [Evaluation Function](#3-evaluation-function)\n",
    "4. [Models](#4-models)\n",
    "5. [Experiment - Llama-2-7B-Chat-GGML](#5-experiment---llama-2-7b-chat-ggml)\n",
    "6. [Overall Results](#9-overall-results)\n",
    "7. [Insights](#10-insights)\n",
    "\n",
    "### Objective\n",
    "\n",
    "The primary goal of this notebook is to test the feasibility of utilising Large Language Model (LLM) for our  multitask objectives of sentiment classification and response generation. `llama-cpp-python` is the Python bindings for `llama.cpp`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from textstat import flesch_reading_ease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "      <th>resp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1783813</th>\n",
       "      <td>2</td>\n",
       "      <td>AYCE Shrimp good value if you like the flavors...</td>\n",
       "      <td>Were so glad you had a great experience at our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035152</th>\n",
       "      <td>2</td>\n",
       "      <td>Dwayne was our salesman and he really put fort...</td>\n",
       "      <td>Hello Barbara, thank you we appreciate your fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2946095</th>\n",
       "      <td>2</td>\n",
       "      <td>Good value on cabinets and flooring .Very nice...</td>\n",
       "      <td>Thank you!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371890</th>\n",
       "      <td>2</td>\n",
       "      <td>Great atmosphere, knowledgeable, and friendly....</td>\n",
       "      <td>Thank you much appreciated brother see you bac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962314</th>\n",
       "      <td>2</td>\n",
       "      <td>Casual atmosphere with great steak and sides. ...</td>\n",
       "      <td>Thank you, Jake! So glad you love our place! W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467452</th>\n",
       "      <td>0</td>\n",
       "      <td>If I could give this office zero stars, I woul...</td>\n",
       "      <td>Were sorry to hear about your negative experie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920144</th>\n",
       "      <td>2</td>\n",
       "      <td>They really get to know your animal! They love...</td>\n",
       "      <td>Hi Kristen Thank you so much for sharing your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3125892</th>\n",
       "      <td>2</td>\n",
       "      <td>Nick is amazing, as well as the rest of the st...</td>\n",
       "      <td>Thank you for your kind review</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80766</th>\n",
       "      <td>2</td>\n",
       "      <td>Best place for equine stuff</td>\n",
       "      <td>Thanks Lisa...we appreciate the FIVE Star Revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3572085</th>\n",
       "      <td>2</td>\n",
       "      <td>No indoor dining. Great food, terrific Jamoca ...</td>\n",
       "      <td>Thank you for reaching out to us.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1271583</th>\n",
       "      <td>2</td>\n",
       "      <td>The goods you come to love</td>\n",
       "      <td>Thank you very much for such a great review! W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663165</th>\n",
       "      <td>2</td>\n",
       "      <td>Nice young lady explained everything. Food coo...</td>\n",
       "      <td>Thanks for letting us know! We will pass that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512529</th>\n",
       "      <td>2</td>\n",
       "      <td>Good food and good service.</td>\n",
       "      <td>Thanks for stopping in!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721375</th>\n",
       "      <td>2</td>\n",
       "      <td>I just started my therapy at Spooner but I can...</td>\n",
       "      <td>Belen, thank you for highly recommending Spoon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259554</th>\n",
       "      <td>1</td>\n",
       "      <td>It was ok</td>\n",
       "      <td>We appreciate your feedback.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254551</th>\n",
       "      <td>2</td>\n",
       "      <td>Very good food and service.</td>\n",
       "      <td>Thank you very much.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760092</th>\n",
       "      <td>2</td>\n",
       "      <td>This establishment has been there for 20 years...</td>\n",
       "      <td>Keith .. I am impressed with you food and liba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836195</th>\n",
       "      <td>2</td>\n",
       "      <td>Little crowded but friendly people</td>\n",
       "      <td>Thanks for the review, Sandra. Well share this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2494501</th>\n",
       "      <td>2</td>\n",
       "      <td>Bought my Sirrus from these guys! Super knowle...</td>\n",
       "      <td>Congrats on the Sirrus Matt, such an awesome b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2676319</th>\n",
       "      <td>2</td>\n",
       "      <td>Great service</td>\n",
       "      <td>We appreciate the feedback, Art! Great service...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         rating                                               text  \\\n",
       "1783813       2  AYCE Shrimp good value if you like the flavors...   \n",
       "2035152       2  Dwayne was our salesman and he really put fort...   \n",
       "2946095       2  Good value on cabinets and flooring .Very nice...   \n",
       "1371890       2  Great atmosphere, knowledgeable, and friendly....   \n",
       "962314        2  Casual atmosphere with great steak and sides. ...   \n",
       "2467452       0  If I could give this office zero stars, I woul...   \n",
       "920144        2  They really get to know your animal! They love...   \n",
       "3125892       2  Nick is amazing, as well as the rest of the st...   \n",
       "80766         2                        Best place for equine stuff   \n",
       "3572085       2  No indoor dining. Great food, terrific Jamoca ...   \n",
       "1271583       2                         The goods you come to love   \n",
       "1663165       2  Nice young lady explained everything. Food coo...   \n",
       "1512529       2                        Good food and good service.   \n",
       "1721375       2  I just started my therapy at Spooner but I can...   \n",
       "1259554       1                                         It was ok    \n",
       "1254551       2                        Very good food and service.   \n",
       "1760092       2  This establishment has been there for 20 years...   \n",
       "1836195       2                 Little crowded but friendly people   \n",
       "2494501       2  Bought my Sirrus from these guys! Super knowle...   \n",
       "2676319       2                                      Great service   \n",
       "\n",
       "                                                      resp  \n",
       "1783813  Were so glad you had a great experience at our...  \n",
       "2035152  Hello Barbara, thank you we appreciate your fe...  \n",
       "2946095                                         Thank you!  \n",
       "1371890  Thank you much appreciated brother see you bac...  \n",
       "962314   Thank you, Jake! So glad you love our place! W...  \n",
       "2467452  Were sorry to hear about your negative experie...  \n",
       "920144   Hi Kristen Thank you so much for sharing your ...  \n",
       "3125892                     Thank you for your kind review  \n",
       "80766    Thanks Lisa...we appreciate the FIVE Star Revi...  \n",
       "3572085                  Thank you for reaching out to us.  \n",
       "1271583  Thank you very much for such a great review! W...  \n",
       "1663165  Thanks for letting us know! We will pass that ...  \n",
       "1512529                            Thanks for stopping in!  \n",
       "1721375  Belen, thank you for highly recommending Spoon...  \n",
       "1259554                       We appreciate your feedback.  \n",
       "1254551                               Thank you very much.  \n",
       "1760092  Keith .. I am impressed with you food and liba...  \n",
       "1836195  Thanks for the review, Sandra. Well share this...  \n",
       "2494501  Congrats on the Sirrus Matt, such an awesome b...  \n",
       "2676319  We appreciate the feedback, Art! Great service...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataframe = pd.read_csv(\"../data/split/test.csv\")\n",
    "subset_test_dataframe = test_dataframe.sample(n=20, random_state=42)\n",
    "subset_test_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sentiment(\n",
    "    df: pd.DataFrame, true_labels_col: str, predicted_labels_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates the sentiment classification performance by calculating \n",
    "    the accuracy and F1 score based on true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the true and predicted \n",
    "            labels.\n",
    "        true_labels_col (str): The name of the column containing the true \n",
    "            labels.\n",
    "        predicted_labels_col (str): The name of the column containing the \n",
    "            predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with added 'Accuracy' and 'F1_Score' \n",
    "            columns.\n",
    "    \"\"\"\n",
    "    df['Accuracy'] = accuracy_score(\n",
    "        df[true_labels_col], df[predicted_labels_col]\n",
    "    )\n",
    "    df['F1_Score'] = f1_score(\n",
    "        df[true_labels_col], df[predicted_labels_col], average='weighted'\n",
    "    )\n",
    "    print(f\"Overall Accuracy: {df['Accuracy'].mean()*100:.2f}%\")\n",
    "    print(f\"Overall F1 Score: {df['F1_Score'].mean()*100:.2f}%\")\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def average_sentence_length(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the average sentence length of a text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        float: The average sentence length.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sum(\n",
    "        len(word_tokenize(sentence)) for sentence in sentences\n",
    "    ) / len(sentences)\n",
    "\n",
    "def average_word_length(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the average word length of a text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        float: The average word length.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "def lexical_diversity(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the lexical diversity of a text.\n",
    "\n",
    "    Parameters:\n",
    "        text (str): The text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        float: The lexical diversity.\n",
    "    \"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "def evaluate_generated_response(\n",
    "    df: pd.DataFrame, response_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates the quality of generated text responses by calculating \n",
    "    readability, fluency, and complexity metrics.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame containing the generated responses.\n",
    "        response_col (str): The name of the column containing the generated \n",
    "            responses.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with added evaluation metrics as columns.\n",
    "    \"\"\"\n",
    "    readability_scores = []\n",
    "    avg_sentence_lengths = []\n",
    "    avg_word_lengths = []\n",
    "    lexical_diversities = []\n",
    "\n",
    "    for response in df[response_col]:\n",
    "        # Evaluate Readability\n",
    "        readability_scores.append(flesch_reading_ease(response))\n",
    "        \n",
    "        # Evaluate Complexity\n",
    "        avg_sentence_lengths.append(average_sentence_length(response))\n",
    "        avg_word_lengths.append(average_word_length(response))\n",
    "        lexical_diversities.append(lexical_diversity(response))\n",
    "\n",
    "    df['Readability_Score'] = readability_scores\n",
    "    df['Avg_Sentence_Length'] = avg_sentence_lengths\n",
    "    df['Avg_Word_Length'] = avg_word_lengths\n",
    "    df['Lexical_Diversity'] = lexical_diversities\n",
    "\n",
    "    # Calculate and print overall scores\n",
    "    print(f\"Overall Readability Score: {df['Readability_Score'].mean():.2f}\")\n",
    "    print(f\"Overall Average Sentence Length: \")\n",
    "    print(f\"{df['Avg_Sentence_Length'].mean():.2f}\")\n",
    "    print(f\"Overall Average Word Length: {df['Avg_Word_Length'].mean():.2f}\")\n",
    "    print(f\"Overall Lexical Diversity: {df['Lexical_Diversity'].mean():.2f}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Models\n",
    "\n",
    "<div style=\"background-color: black; color: white; padding: 10px\">\n",
    "    <p><b>In this notebook, I will experiment with a couple of the available models to test the prompt template as well as finalize any post-processing steps required:\n",
    "\n",
    "Llama2-7B Chat\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment - Llama-2-7B-Chat-GGML\n",
    "\n",
    "First, we experiment with LlaMA2 model from Meta AI. It is fine-tuned for dialogue.\n",
    "\n",
    "**Hugging Face Model card** - https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../models/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - kv   0:                       general.architecture str     \n",
      "llama_model_loader: - kv   1:                               general.name str     \n",
      "llama_model_loader: - kv   2:                       llama.context_length u32     \n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32     \n",
      "llama_model_loader: - kv   4:                          llama.block_count u32     \n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32     \n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32     \n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \n",
      "llama_model_loader: - kv  10:                          general.file_type u32     \n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str     \n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr     \n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr     \n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr     \n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32     \n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32     \n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32     \n",
      "llama_model_loader: - kv  18:               general.quantization_version u32     \n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_print_meta: format         = GGUF V2 (latest)\n",
      "llm_load_print_meta: arch           = llama\n",
      "llm_load_print_meta: vocab type     = SPM\n",
      "llm_load_print_meta: n_vocab        = 32000\n",
      "llm_load_print_meta: n_merges       = 0\n",
      "llm_load_print_meta: n_ctx_train    = 4096\n",
      "llm_load_print_meta: n_ctx          = 2048\n",
      "llm_load_print_meta: n_embd         = 4096\n",
      "llm_load_print_meta: n_head         = 32\n",
      "llm_load_print_meta: n_head_kv      = 32\n",
      "llm_load_print_meta: n_layer        = 32\n",
      "llm_load_print_meta: n_rot          = 128\n",
      "llm_load_print_meta: n_gqa          = 1\n",
      "llm_load_print_meta: f_norm_eps     = 1.0e-05\n",
      "llm_load_print_meta: f_norm_rms_eps = 1.0e-06\n",
      "llm_load_print_meta: n_ff           = 11008\n",
      "llm_load_print_meta: freq_base      = 10000.0\n",
      "llm_load_print_meta: freq_scale     = 1\n",
      "llm_load_print_meta: model type     = 7B\n",
      "llm_load_print_meta: model ftype    = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model size     = 6.74 B\n",
      "llm_load_print_meta: general.name   = LLaMA v2\n",
      "llm_load_print_meta: BOS token = 1 '<s>'\n",
      "llm_load_print_meta: EOS token = 2 '</s>'\n",
      "llm_load_print_meta: UNK token = 0 '<unk>'\n",
      "llm_load_print_meta: LF token  = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.09 MB\n",
      "llm_load_tensors: mem required  = 3891.34 MB (+ 1024.00 MB per state)\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: kv self size  = 1024.00 MB\n",
      "llama_new_context_with_model: compute buffer total size =  153.47 MB\n",
      "AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model = Llama(\"../models/llama-2-7b-chat.Q4_K_M.gguf\", verbose=True, n_ctx=2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompt function to perform Sentiment Classification and Response Generation Task**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_llama_prompt(review: str) -> str:\n",
    "    prompt = (\n",
    "        \"[INST] <<SYS>>\\n\"\n",
    "        \"You are a helpful and attentive business owner who always \"\n",
    "        \"appreciates customer feedback. If the customer had a positive \"\n",
    "        \"experience, make sure to thank them. If the customer had a \"\n",
    "        \"negative or neutral experience, apologize and address their \"\n",
    "        \"concerns respectfully and constructively. Additionally, try to \"\n",
    "        \"identify any potential improvements or criticisms mentioned in the review. \"\n",
    "        \"Do limit your response to 3 sentences.\\n\"\n",
    "        \"<</SYS>>\\n\\n\"\n",
    "        \"Please classify the following customer review as either \"\n",
    "        \"\\\"Positive,\\\" \\\"Negative,\\\" or \\\"Neutral.\\\" Then, provide a business owner's \"\n",
    "        \"response accordingly. Also, explicitly identify any potential improvements \"\n",
    "        \"or criticisms mentioned in the review, filling them in the corresponding \"\n",
    "        \"fields. If none are mentioned, write \\\"None\\\" for each field.\\n\"\n",
    "        f\"The customer review is: \\\"{review}\\\"\\n\"\n",
    "        \"Strictly format and return your output in the following JSON format: \"\n",
    "        \"{\\\"sentiment\\\": \\\"<Sentiment>\\\", \\\"response\\\": \\\"<Response>\\\", \"\n",
    "        \"\\\"improvements\\\": \\\"<Improvements>\\\", \\\"criticisms\\\": \\\"<Criticisms>\\\"}\\n\"\n",
    "        \"[/INST]\\n\\n \"\n",
    "    )\n",
    "\n",
    "    output = model(prompt, max_tokens=150)\n",
    "    return output\n",
    "\n",
    "\n",
    "def try_use_llama_prompt(text):\n",
    "    max_attempts = 3\n",
    "    for attempt in range(max_attempts):\n",
    "        output = use_llama_prompt(text)\n",
    "        try:\n",
    "            evaluated_output = eval(output['choices'][0]['text'])\n",
    "            return evaluated_output\n",
    "        except:\n",
    "            print(\n",
    "                f\"Attempt {attempt+1} failed: output was gibberish. Trying again.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    66.42 ms /    94 runs   (    0.71 ms per token,  1415.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6498.29 ms /   137 tokens (   47.43 ms per token,    21.08 tokens per second)\n",
      "llama_print_timings:        eval time =  4927.03 ms /    93 runs   (   52.98 ms per token,    18.88 tokens per second)\n",
      "llama_print_timings:       total time = 11608.65 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    60.65 ms /    86 runs   (    0.71 ms per token,  1418.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6216.45 ms /   105 tokens (   59.20 ms per token,    16.89 tokens per second)\n",
      "llama_print_timings:        eval time =  4513.90 ms /    85 runs   (   53.10 ms per token,    18.83 tokens per second)\n",
      "llama_print_timings:       total time = 10900.01 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: output was gibberish. Trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    55.70 ms /    79 runs   (    0.71 ms per token,  1418.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5739.95 ms /    74 tokens (   77.57 ms per token,    12.89 tokens per second)\n",
      "llama_print_timings:        eval time =  4103.00 ms /    78 runs   (   52.60 ms per token,    19.01 tokens per second)\n",
      "llama_print_timings:       total time =  9999.69 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    81.81 ms /   116 runs   (    0.71 ms per token,  1417.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  6153.38 ms /   116 runs   (   53.05 ms per token,    18.85 tokens per second)\n",
      "llama_print_timings:       total time =  6382.07 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: output was gibberish. Trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =   105.94 ms /   150 runs   (    0.71 ms per token,  1415.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6250.80 ms /    99 tokens (   63.14 ms per token,    15.84 tokens per second)\n",
      "llama_print_timings:        eval time =  8026.16 ms /   149 runs   (   53.87 ms per token,    18.56 tokens per second)\n",
      "llama_print_timings:       total time = 14579.62 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 2 failed: output was gibberish. Trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    70.01 ms /    99 runs   (    0.71 ms per token,  1414.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5284.25 ms /    99 runs   (   53.38 ms per token,    18.73 tokens per second)\n",
      "llama_print_timings:       total time =  5480.60 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    64.94 ms /    92 runs   (    0.71 ms per token,  1416.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  4902.69 ms /    92 runs   (   53.29 ms per token,    18.77 tokens per second)\n",
      "llama_print_timings:       total time =  5085.89 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    97.64 ms /   135 runs   (    0.72 ms per token,  1382.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6119.73 ms /   101 tokens (   60.59 ms per token,    16.50 tokens per second)\n",
      "llama_print_timings:        eval time =  7486.68 ms /   134 runs   (   55.87 ms per token,    17.90 tokens per second)\n",
      "llama_print_timings:       total time = 13891.05 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    89.10 ms /   126 runs   (    0.71 ms per token,  1414.19 tokens per second)\n",
      "llama_print_timings: prompt eval time = 12242.54 ms /   522 tokens (   23.45 ms per token,    42.64 tokens per second)\n",
      "llama_print_timings:        eval time =  7285.29 ms /   125 runs   (   58.28 ms per token,    17.16 tokens per second)\n",
      "llama_print_timings:       total time = 19787.73 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: output was gibberish. Trying again.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    67.98 ms /    96 runs   (    0.71 ms per token,  1412.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5668.50 ms /    86 tokens (   65.91 ms per token,    15.17 tokens per second)\n",
      "llama_print_timings:        eval time =  5033.69 ms /    95 runs   (   52.99 ms per token,    18.87 tokens per second)\n",
      "llama_print_timings:       total time = 10894.61 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    77.56 ms /   109 runs   (    0.71 ms per token,  1405.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =  5819.78 ms /   109 runs   (   53.39 ms per token,    18.73 tokens per second)\n",
      "llama_print_timings:       total time =  6039.17 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    79.81 ms /   113 runs   (    0.71 ms per token,  1415.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6459.19 ms /   110 tokens (   58.72 ms per token,    17.03 tokens per second)\n",
      "llama_print_timings:        eval time =  6009.88 ms /   112 runs   (   53.66 ms per token,    18.64 tokens per second)\n",
      "llama_print_timings:       total time = 12696.55 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    69.09 ms /    98 runs   (    0.70 ms per token,  1418.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5902.71 ms /    67 tokens (   88.10 ms per token,    11.35 tokens per second)\n",
      "llama_print_timings:        eval time =  5102.71 ms /    97 runs   (   52.61 ms per token,    19.01 tokens per second)\n",
      "llama_print_timings:       total time = 11202.27 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    62.80 ms /    89 runs   (    0.71 ms per token,  1417.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5975.13 ms /    89 tokens (   67.14 ms per token,    14.90 tokens per second)\n",
      "llama_print_timings:        eval time =  4662.29 ms /    88 runs   (   52.98 ms per token,    18.87 tokens per second)\n",
      "llama_print_timings:       total time = 10817.49 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    57.18 ms /    81 runs   (    0.71 ms per token,  1416.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5864.23 ms /    67 tokens (   87.53 ms per token,    11.43 tokens per second)\n",
      "llama_print_timings:        eval time =  4208.45 ms /    80 runs   (   52.61 ms per token,    19.01 tokens per second)\n",
      "llama_print_timings:       total time = 10236.15 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    72.07 ms /   102 runs   (    0.71 ms per token,  1415.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5877.95 ms /    74 tokens (   79.43 ms per token,    12.59 tokens per second)\n",
      "llama_print_timings:        eval time =  5335.74 ms /   101 runs   (   52.83 ms per token,    18.93 tokens per second)\n",
      "llama_print_timings:       total time = 11420.52 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    73.48 ms /   104 runs   (    0.71 ms per token,  1415.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5610.47 ms /    66 tokens (   85.01 ms per token,    11.76 tokens per second)\n",
      "llama_print_timings:        eval time =  5427.39 ms /   103 runs   (   52.69 ms per token,    18.98 tokens per second)\n",
      "llama_print_timings:       total time = 11248.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    84.07 ms /   119 runs   (    0.71 ms per token,  1415.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  6264.63 ms /    99 tokens (   63.28 ms per token,    15.80 tokens per second)\n",
      "llama_print_timings:        eval time =  6333.21 ms /   118 runs   (   53.67 ms per token,    18.63 tokens per second)\n",
      "llama_print_timings:       total time = 12838.96 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    59.30 ms /    84 runs   (    0.71 ms per token,  1416.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5372.23 ms /    64 tokens (   83.94 ms per token,    11.91 tokens per second)\n",
      "llama_print_timings:        eval time =  4348.94 ms /    83 runs   (   52.40 ms per token,    19.09 tokens per second)\n",
      "llama_print_timings:       total time =  9890.35 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    60.83 ms /    86 runs   (    0.71 ms per token,  1413.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5446.42 ms /    67 tokens (   81.29 ms per token,    12.30 tokens per second)\n",
      "llama_print_timings:        eval time =  4574.12 ms /    85 runs   (   53.81 ms per token,    18.58 tokens per second)\n",
      "llama_print_timings:       total time = 10195.13 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    74.11 ms /   105 runs   (    0.71 ms per token,  1416.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =  8011.47 ms /   222 tokens (   36.09 ms per token,    27.71 tokens per second)\n",
      "llama_print_timings:        eval time =  5771.29 ms /   104 runs   (   55.49 ms per token,    18.02 tokens per second)\n",
      "llama_print_timings:       total time = 13999.01 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    40.36 ms /    57 runs   (    0.71 ms per token,  1412.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5990.70 ms /    68 tokens (   88.10 ms per token,    11.35 tokens per second)\n",
      "llama_print_timings:        eval time =  2937.58 ms /    56 runs   (   52.46 ms per token,    19.06 tokens per second)\n",
      "llama_print_timings:       total time =  9044.24 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    63.51 ms /    90 runs   (    0.71 ms per token,  1417.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5948.03 ms /    76 tokens (   78.26 ms per token,    12.78 tokens per second)\n",
      "llama_print_timings:        eval time =  4694.48 ms /    89 runs   (   52.75 ms per token,    18.96 tokens per second)\n",
      "llama_print_timings:       total time = 10825.11 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  8157.74 ms\n",
      "llama_print_timings:      sample time =    56.41 ms /    80 runs   (    0.71 ms per token,  1418.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =  5208.97 ms /    64 tokens (   81.39 ms per token,    12.29 tokens per second)\n",
      "llama_print_timings:        eval time =  4135.38 ms /    79 runs   (   52.35 ms per token,    19.10 tokens per second)\n",
      "llama_print_timings:       total time =  9505.96 ms\n"
     ]
    }
   ],
   "source": [
    "subset_test_dataframe['output'] = subset_test_dataframe['text'].apply(\n",
    "    try_use_llama_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post Processing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['sentiment']\n",
    ")\n",
    "subset_test_dataframe['response'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['response']\n",
    ")\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['improvements']\n",
    ")\n",
    "subset_test_dataframe['response'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['criticisms']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print output samples:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0\n",
      "\n",
      "AYCE Shrimp good value if you like the flavors they seve but sevice was ok\n",
      "\n",
      "\n",
      "{'sentiment': 'Neutral', 'response': 'Thank you for taking the time to share your feedback about your experience at our AYCE Shrimp station. We apologize that the service did not meet your expectations, and we will take this feedback into consideration to improve our services in the future. We appreciate your loyalty and hope to serve you better next time.', 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 1\n",
      "\n",
      "Dwayne was our salesman and he really put forth the extra effort to make sure we were satisfied with our purchase. They really had A LOT of quality cars trucks, etc. to choose from !\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': \"Thank you for taking the time to share your feedback! We're glad to hear that Dwayne provided excellent service and helped you find a quality vehicle. We appreciate your kind words and will continue to strive for excellence in our customer service. \", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 2\n",
      "\n",
      "Good value on cabinets and flooring .Very nice staff.\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': \"Thank you for taking the time to leave a review of your experience with us! We're glad to hear that you found good value on our cabinets and flooring, and we appreciate your kind words about our staff. We strive to provide excellent service and quality products, and it's great to see that our efforts are paying off. Thank you for choosing us as your supplier!\", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 3\n",
      "\n",
      "Great atmosphere, knowledgeable, and friendly. With so many handsome men walking out of the shop, I can see why the OwnerBarber always has a smile on his face!\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to leave us a review! We're thrilled to hear that you enjoyed your experience at our shop, especially the great atmosphere and knowledgeable staff. We'll be sure to share your kind words with the team. \", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 4\n",
      "\n",
      "Casual atmosphere with great steak and sides. I met one of the owners and he was very kind. I went here with my family for a celebration and everyone had a great time!\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to review your experience at our restaurant! We're thrilled to hear that you enjoyed the casual atmosphere and the quality of our steak and sides. It's great to hear that you had a positive interaction with one of our owners, and we appreciate your feedback on our family-friendly environment. We'll continue to work hard to provide an exceptional dining experience for all of our guests. Thank you again for your kind words!\", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 5\n",
      "\n",
      "If I could give this office zero stars, I would. A year ago, we visited the office hoping to get some assistance in planning an itinerary that would involve air and train travel. The person in the AAA office who specialized in this knew nothing at all about Amtrack and could only offer us the web address. Flashforward to this year. We stopped in the office to request a TripTik for a planned drive from Orange, MA to Wildwood, NJ. We were told the one person in the office capable of mapping out the route was off that day, but the receptionist took our information and said the responsible party would call us the following morning. We left instructions for the trip an area of Connecticut we wanted to avoid because we knew of major construction and that we wanted to cross the Hudson via the Tappanzee Bridge rather than the George Washington. We had visited the office on a Wednesday. When we hadnt heard from AAA by Monday morning, we called the office and spoke with the person who is responsible for creating TripTiks. She found the receptionists note, but hadnt begun working on it yet. I explained we were leaving very early Wednesday morning, so I needed to pick up the information Tuesday. She assured me all would be ready. I asked if she would also provide a travel book for New Jersey and paper maps for all the states wed be going through and she agreed all would be together for us when we arrived at the office. My husband went to the office in the middle of Tuesday afternoon. The TripTik wasnt bound yet, so he had to wait for that to be done. When he asked about the maps and tour book, the woman pointed toward a wall and said, Theyre over there. The TripTik amounted to nothing more than segments of maps that could have been printed from Mapquest. Once upon a time, such a routing document from AAA would include information about restaurants, etc., on each segment of the trip, but apparently thats no longer the case. This document along with the staff of this office proved to be utterly USELESS. If I didnt need the roadside services, Id drop my membership.\n",
      "\n",
      "\n",
      "{'sentiment': 'Negative', 'response': 'We apologize for the negative experience you had at our office. We understand that you were hoping to get helpful planning assistance and were disappointed with the service you received. We will review this matter internally and take steps to improve our processes to better serve our customers in the future.', 'improvements': ['Providing more detailed and accurate information in the TripTik', 'Offering more comprehensive travel guides for New Jersey'], 'criticisms': ['Lack of knowledge about Amtrak, unhelpful staff']}\n",
      "\n",
      "\n",
      "Index 6\n",
      "\n",
      "They really get to know your animal! They love it as if its their own! There pet plans make the experience much easier!\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': 'Thank you so much for taking the time to leave us a review! We are thrilled to hear that our team has been able to provide exceptional care for your furry friend. Your feedback is invaluable, and we will continue to strive to exceed your expectations. We appreciate your loyalty and look forward to serving you and your pet in the future!', 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 7\n",
      "\n",
      "Nick is amazing, as well as the rest of the stuff. Very competent in the content that they are selling. Great energy! Would be a 5 if they had what I needed in stock. Will continue to shop here!\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to leave us a review! We're thrilled to hear that Nick and the rest of our team provided excellent service. Sorry to hear that we didn't have what you needed in stock, but we appreciate your continued support. We will make sure to keep a wider selection of products available in the future. Thank you again for your feedback!\", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 8\n",
      "\n",
      "Best place for equine stuff\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': 'Thank you so much for taking the time to leave such a kind review! We are thrilled to hear that you found our store to be the best place for equine stuff. Your feedback means everything to us and we will continue to strive to provide excellent service and products. Thank you again for your business!', 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 9\n",
      "\n",
      "No indoor dining. Great food, terrific Jamoca Milkshakes! Did I say No Indoor Dining!!!\n",
      "\n",
      "\n",
      "{'sentiment': 'Negative', 'response': 'We apologize for any inconvenience caused by the lack of indoor dining. We appreciate your positive feedback on our food and drinks. Please know that we take all customer feedback seriously and will consider improving our indoor dining area in the future.', 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 10\n",
      "\n",
      "The goods you come to love\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': 'Thank you so much for taking the time to leave us with a positive review! We are thrilled to hear that you have come to love our goods. We will continue to work hard to ensure that our products meet your high standards.', 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 11\n",
      "\n",
      "Nice young lady explained everything. Food cooked right as well\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to review our business! We're thrilled to hear that you had a positive experience with our team, especially our friendly and knowledgeable young lady. We appreciate your feedback and will continue to strive for excellence in everything we do. Please come back soon and let us serve you again!\", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 12\n",
      "\n",
      "Good food and good service.\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to leave us a review! We're thrilled to hear that you enjoyed your experience with us. Your feedback is invaluable in helping us continue to improve and provide excellent service to our customers. Please don't hesitate to reach out if there's anything else we can do for you.\", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 13\n",
      "\n",
      "I just started my therapy at Spooner but I can tell this place is a cut above the rest. Great customer experience from the moment you schedule an appointment. I highly recommend them!\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': 'Thank you so much for taking the time to share your kind review of our business! We are thrilled to hear that you had a great customer experience from the moment you scheduled your appointment. Your feedback is invaluable to us, and we appreciate your recommendation. We will continue to strive for excellence in providing top-notch therapy services to our clients. Thank you again for choosing Spooner!', 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 14\n",
      "\n",
      "It was ok \n",
      "\n",
      "\n",
      "{'sentiment': 'Neutral', 'response': \"Thank you for taking the time to share your experience with us. We apologize that it was only 'ok' for you, and we appreciate your feedback. We will use this information to make improvements to better serve our customers in the future.\", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 15\n",
      "\n",
      "Very good food and service.\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to review our restaurant! We're thrilled to hear that you enjoyed the food and service. Your feedback is invaluable to us, and we'll continue to work hard to provide an exceptional dining experience for all of our guests.\", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 16\n",
      "\n",
      "This establishment has been there for 20 years yet the actual bar is dated from 1910. The food here was very well prepared. I ordered trout because it is the easiest dish to make and the hardest dish to make to do it well. It was a great dish. The mixologists were extraordinary. Their knowledge of all things libation wise was very impressive. I met only one equal bartender which was Chris at Catherine Lombardi in New Brunswick. Dirty martinis are easy to make. Try asking for a mai tai. The best bartenders know how to raise this cocktail to a high level. I think that Jordy from Verve and Chris formerly from Catherine Lombardi should have a throwdown.\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to share your review with us! We're thrilled to hear that you enjoyed the food and drinks at our establishment, particularly the trout dish. We appreciate your compliments on our mixologists and their knowledge of libations. We will definitely pass along your feedback to our team. Thank you again for your kind words!\", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 17\n",
      "\n",
      "Little crowded but friendly people\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': 'Thank you for taking the time to share your feedback! We appreciate your kind words about the friendly atmosphere of our establishment. Please come back soon!', 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 18\n",
      "\n",
      "Bought my Sirrus from these guys! Super knowledgeable and friendly staff.\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to share your positive experience with us! We're thrilled to hear that our staff was able to provide you with the knowledge and friendliness you expected. Your feedback means a lot to us, and we appreciate your support. \", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n",
      "Index 19\n",
      "\n",
      "Great service\n",
      "\n",
      "\n",
      "{'sentiment': 'Positive', 'response': \"Thank you so much for taking the time to leave a review! We're thrilled to hear that you had a great experience with us. Your satisfaction is our top priority, and we will continue to strive for excellence in everything we do.\", 'improvements': None, 'criticisms': None}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(subset_test_dataframe)):\n",
    "    print(f\"Index {i}\\n\")\n",
    "    print(subset_test_dataframe['text'].iloc[i])\n",
    "    print(\"\\n\")\n",
    "    print(subset_test_dataframe['output'].iloc[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 90.00%\n",
      "Overall F1 Score: 85.26%\n",
      "Overall Readability Score: 79.55\n",
      "Overall Average Sentence Length: \n",
      "15.99\n",
      "Overall Average Word Length: 3.99\n",
      "Overall Lexical Diversity: 0.81\n"
     ]
    }
   ],
   "source": [
    "# SENTIMENT_MAPPING = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "SENTIMENT_MAPPING = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2}\n",
    "\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['sentiment']\n",
    ")\n",
    "subset_test_dataframe['sentiment'] = subset_test_dataframe['sentiment'].map(\n",
    "    SENTIMENT_MAPPING\n",
    ")\n",
    "subset_test_dataframe['response'] = subset_test_dataframe['output'].apply(\n",
    "    lambda x: x['response']\n",
    ")\n",
    "subset_test_dataframe = evaluate_sentiment(\n",
    "    subset_test_dataframe, 'rating', 'sentiment'\n",
    ")\n",
    "subset_test_dataframe = evaluate_generated_response(\n",
    "    subset_test_dataframe, 'response'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Few-shot Prompt function to perform Sentiment Classification and Response Generation Task**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_llama_prompt(review: str) -> str:\n",
    "    prompt = (\n",
    "        \"[INST] <<SYS>>\\n\"\n",
    "        \"You are a helpful and attentive business owner who always \"\n",
    "        \"appreciates customer feedback. Your task is to categorize customer \"\n",
    "        \"reviews as 'Positive,' 'Neutral,' or 'Negative,' and respond accordingly. \"\n",
    "        \"Additionally, identify any potential improvements or criticisms mentioned in \"\n",
    "        \"the review. Limit your response to 3 sentences. \"\n",
    "        \"If no improvements or criticisms are found, specify 'None' for each field.\\n\"\n",
    "        \"<</SYS>>\\n\\n\"\n",
    "        \"-- Example 1 --\\n\"\n",
    "        \"Customer Review: The food was excellent and the service was top-notch.\\n\"\n",
    "        \"Output: {\\\"sentiment\\\": \\\"Positive\\\", \\\"response\\\": \\\"Thank you for your kind words! \"\n",
    "        \"We're glad you enjoyed your experience. Come back soon!\\\", \\\"improvements\\\": \\\"None\\\", \\\"criticisms\\\": \\\"None\\\"}\\n\\n\"\n",
    "        \n",
    "        \"-- Example 2 --\\n\"\n",
    "        \"Customer Review: The service was slow and the food was mediocre.\\n\"\n",
    "        \"Output: {\\\"sentiment\\\": \\\"Negative\\\", \\\"response\\\": \\\"We're sorry to hear about your \"\n",
    "        \"experience. We'll work on our speed and food quality.\\\", \\\"improvements\\\": \\\"Speed up service, Improve food quality\\\", \"\n",
    "        \"\\\"criticisms\\\": \\\"Slow service, Mediocre food\\\"}\\n\\n\"\n",
    "        \n",
    "        \"-- Example 3 --\\n\"\n",
    "        \"Customer Review: The food was good, but the music was too loud.\\n\"\n",
    "        \"Output: {\\\"sentiment\\\": \\\"Neutral\\\", \\\"response\\\": \\\"Thank you for your feedback. \"\n",
    "        \"We're glad you liked the food but sorry the music was too loud.\\\", \\\"improvements\\\": \\\"Adjust music volume\\\", \\\"criticisms\\\": \\\"Loud music\\\"}\\n\\n\"\n",
    "        \n",
    "        \"Please classify the following customer review and provide a business owner's \"\n",
    "        f\"response accordingly. The customer review is: \\\"{review}\\\"\\n\"\n",
    "        \"Strictly format and return your output in the following JSON format: \"\n",
    "        \"{\\\"sentiment\\\": \\\"<Sentiment>\\\", \\\"response\\\": \\\"<Response>\\\", \"\n",
    "        \"\\\"improvements\\\": \\\"<Improvements>\\\", \\\"criticisms\\\": \\\"<Criticisms>\\\"}\\n\"\n",
    "        \"[/INST]\\n\\n \"\n",
    "    )\n",
    "\n",
    "    output = model(prompt, max_tokens=150)\n",
    "    return output\n",
    "\n",
    "\n",
    "def try_use_llama_prompt(text):\n",
    "    max_attempts = 3\n",
    "    for attempt in range(max_attempts):\n",
    "        output = use_llama_prompt(text)\n",
    "        try:\n",
    "            evaluated_output = eval(output['choices'][0]['text'])\n",
    "            return evaluated_output\n",
    "        except:\n",
    "            print(\n",
    "                f\"Attempt {attempt+1} failed: output was gibberish. Trying again.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Overall Results\n",
    "\n",
    "**LlaMA2-7B Chat - GGML Original 4-bit Quantized:**\n",
    "\n",
    "Overall Accuracy: `85.00%`   \n",
    "Overall F1 Score: `87.65%`     \n",
    "Overall Readability Score: `76.75` (Flesch Reading Ease Score - 0 to 100)    \n",
    "Overall Average Sentence Length: `16.98` words per sentence   \n",
    "Overall Average Word Length: `4.05` Characters per Word    \n",
    "Overall Lexical Diversity: `0.81` (Lexical Diversity Ratio - 0 to 1)   \n",
    "Time taken: `4 minutes 45 seconds`\n",
    "\n",
    "**LlaMA2-7B Chat - GGUF 4-bit Quantized [`Zero-Shot`]:**\n",
    "\n",
    "Overall Accuracy: `90.00%`   \n",
    "Overall F1 Score: `85.26%`     \n",
    "Overall Readability Score: `76.02` (Flesch Reading Ease Score - 0 to 100)    \n",
    "Overall Average Sentence Length: `16.54` words per sentence   \n",
    "Overall Average Word Length: `4.04` Characters per Word    \n",
    "Overall Lexical Diversity: `0.83` (Lexical Diversity Ratio - 0 to 1)   \n",
    "Time taken: `2 minutes 53 seconds`\n",
    "\n",
    "**LlaMA2-7B Chat - GGUF 4-bit Quantized [`Few-Shot`]:**\n",
    "\n",
    "Overall Accuracy: `%`   \n",
    "Overall F1 Score: `%`     \n",
    "Overall Readability Score: `` (Flesch Reading Ease Score - 0 to 100)     \n",
    "Overall Average Sentence Length: `` words per sentence     \n",
    "Overall Average Word Length: `` Characters per Word      \n",
    "Overall Lexical Diversity: `` (Lexical Diversity Ratio - 0 to 1)     \n",
    "Time taken: ``   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mt-sentigen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
